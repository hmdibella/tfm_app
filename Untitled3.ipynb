{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01a1b7c-e3cf-4b08-8bc3-42b55b482ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import warnings\n",
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "load_dotenv() # Loads .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46af5f1d-18e5-4460-bae0-3c5467e79b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2238e883-4254-48f2-bc05-274b286c76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"\"\n",
    "MODEL_EMBEDDINGS_GOOGLE = \"models/embedding-001\"\n",
    "MODEL_LLM_GOOGLE = \"gemini-1.5-pro\"\n",
    "OUTPUT_PATH = \"output\"\n",
    "FAISS_GOOGLE_PATH = \"output/faiss_index\"\n",
    "NUMEXPR_MAX_THREADS = \"16\"\n",
    "TASK_TYPES = [\"task_type_unspecified\", \"retrieval_query\", \"retrieval_document\", \"semantic_similarity\", \"classification\", \"clustering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a4df34-9044-4c89-abac-d751ef525b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTUALIZE_Q_SYSTEM_PROMPT = \"\"\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d12efca-9496-4dc8-8b25-be4a75ec2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_GOOGLE = \"\"\"You are an intelligent assistant. Answer the question based solely on the information provided in the context, specific for Spain.\n",
    "Do not add any information beyond what is given. \n",
    "If the context does not contain the answer, respond with: \"The answer is not available in the provided context.\"\n",
    "Be concise and accurate. All the answers must be in Spanish language from Spain, in an informal manner.\n",
    "Answers must be clear, precise, and unambiguous, and a teenager should be able to understand the answer.\n",
    "Explicitly avoid phrases such as \"según el documento\", \"según el capítulo\", \"en el texto\", \"como se menciona en el artículo\", or any implication of external texts. Do not construct questions that require knowledge of the structure of the document or the location of information in it.\n",
    "Include the content-specific information that supports the answer to allow the answer to be independent of any external text.\n",
    "If the content lacks sufficient information to form a complete answer, do not force one.\n",
    "Create the answers in your own words; Direct copying of content is not permitted.\n",
    "NEVER mention the words \"documento\", \"texto\", \"presentación\", \"archivo\", \"tabla\", \"artículo\", \"ley\", \"capítulo\", \"preámbulo\", \"título preliminar\", \"disposición\" or \"disposiciones generales\" in your questions or answers.\n",
    "ALWAYS make sure that all answers are accurate, self-contained, and relevant, without relying on any original document or text or implying its existence, strictly avoiding any invention or speculation.\n",
    "IMPORTANT: if in the question there is no mention of a Comunidad Autonoma or the name of a city or province, try that the answer applies to Spain as a country.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b0005c-8a5a-49ba-9fb3-c5031d58ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    global GOOGLE_API_KEY\n",
    "    global MODEL_EMBEDDINGS_GOOGLE\n",
    "    global OUTPUT_PATH\n",
    "    global FAISS_GOOGLE_PATH\n",
    "    global LOG_PATH\n",
    "    global NUMEXPR_MAX_THREADS\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('streamlit_google_history_final.ini')\n",
    "    GOOGLE_API_KEY = config['KEYS']['google_api_key']\n",
    "    MODEL_EMBEDDINGS_GOOGLE = config['MODELS']['model_embeddings_google']\n",
    "    MODEL_LLM_GOOGLE = config['MODELS']['model_llm_google']\n",
    "    OUTPUT_PATH = config['DEFAULT']['output_path']\n",
    "    FAISS_GOOGLE_PATH = config['DEFAULT']['faiss_google_path']\n",
    "    NUMEXPR_MAX_THREADS = config['DEFAULT']['numexpr_max_threads']\n",
    "    LOG_PATH = config['DEFAULT']['log_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62015cd7-dd2c-41f5-9661-8454d4fde2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "load_config()\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = NUMEXPR_MAX_THREADS\n",
    "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
    "os.environ['TAVILY_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615cebbb-fcc6-4ac9-b950-df5f155320ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0f5cd8-0e7b-4251-ac48-ef0dd2a79e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    global store\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f521fd-ffe2-4bc7-8963-c5f48837fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain(retriever):\n",
    "    \n",
    "    llm = ChatGoogleGenerativeAI(model = MODEL_LLM_GOOGLE, temperature = 0.3)\n",
    "    \n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", CONTEXTUALIZE_Q_SYSTEM_PROMPT),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "    \n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", PROMPT_TEMPLATE_GOOGLE),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    \n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    \n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45468f9-275d-4206-b13f-9c5a7a866f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(user_question, k_value, session_id):\n",
    "    #embeddings = GoogleGenerativeAIEmbeddings(model = MODEL_EMBEDDINGS_GOOGLE, task_type=task_type)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-xlm-r-multilingual-v1\")\n",
    "    #new_db = FAISS.load_local(FAISS_GOOGLE_PATH + \"_\" + task_type, embeddings, allow_dangerous_deserialization=True)\n",
    "    new_db = FAISS.load_local(\"output/faiss_index_ollama\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = new_db.as_retriever(search_kwargs={\"k\": k_value})\n",
    "    chain = get_conversational_chain(retriever)\n",
    "    response = chain.invoke(\n",
    "        {\"input\": user_question},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": session_id}\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93525169-437d-4374-8059-aced394e8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generator(prompt, k_value, session_id):\n",
    "    start_model_exec = time.time()\n",
    "    response = user_input(prompt, k_value, session_id)\n",
    "    end_model_exec = time.time()\n",
    "    resp_text = \"{0} (Tiempo de respuesta: {1:.2f} seg. Session ID: {2}).\".format(response, end_model_exec - start_model_exec,  session_id)\n",
    "    for word in resp_text.split():\n",
    "        yield word + \" \"\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b52cd8d8-6bc6-45bd-b6dc-7ba004ebafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "957a4913-7f08-4114-8df1-6c2d2b43a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe46140-3b5c-4b15-92a7-1fbb5cf45ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-xlm-r-multilingual-v1\")\n",
    "    new_db = FAISS.load_local(\"output/faiss_index_ollama\", embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = new_db.as_retriever(search_kwargs={\"k\": k_value})\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049c5cc3-d38c-4849-902d-83645007e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"paraphrase-xlm-r-multilingual-v1\")\n",
    "    new_db = FAISS.from_documents(documents, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = new_db.as_retriever(search_kwargs={\"k\": k_value})\n",
    "    chain = get_conversational_chain(retriever)\n",
    "    generation = chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": session_id}\n",
    "        },\n",
    "    )    \n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8887054-3efe-4779-a15d-42cdeb89ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model = MODEL_LLM_GOOGLE, temperature = 0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a user question to a vectorstore or web search. Use the vectorstore for questions on \n",
    "    \"Código de Tráfico y Seguridad Vial en España\". You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Otherwise, use web-search. Give a binary choice 'web_search' or 'vectorstore' based on the question. Return the a JSON with a single key \n",
    "    'datasource' and no premable or explanation. \n",
    "    \n",
    "    Question to route: {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "    if source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3af6d34-f37d-4f51-a5e7-3db803ccf726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model = MODEL_LLM_GOOGLE, temperature = 0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n",
    "    Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    \n",
    "    Here is the answer: {generation}\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. Give a binary score 'yes' or 'no' to \n",
    "    indicate whether the answer is useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and \n",
    "    no preamble or explanation.\n",
    "    \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    \n",
    "    Here is the question: {question}\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08832f0-c591-44ad-8b65-d3bedad55c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model = MODEL_LLM_GOOGLE, temperature = 0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. If the document \n",
    "    contains keywords related to the user question, grade it as relevant. It does not need to be a stringent test. \n",
    "    The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "    \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    \n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b66bbc2b-3649-4d93-9783-f229c7fa4826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search = state[\"web_search\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcde6f-5d4c-40ce-8781-57f04ed59b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfb5394a-54f5-4af8-8cba-08b858d9dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a71d130-be9a-426e-9deb-63a3592a70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06a1f4e9-cc68-40c1-b3da-bdaa2939acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 8\n",
    "session_id = \"hdb00011\"\n",
    "prompt = \"cual es la temperatura en Madrid?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb7b4d-87b6-4226-938a-993338f106b5",
   "metadata": {},
   "source": [
    "response = {}\n",
    "for task in TASK_TYPES:\n",
    "    response[task] = user_input(prompt, k_value, session_id, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8b460-6f72-4600-80a1-8688e9671e55",
   "metadata": {},
   "source": [
    "for task in TASK_TYPES:\n",
    "    print(task)\n",
    "    print(response[task]['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c7d02-bfb8-48f8-a7f2-1f1a3a54b320",
   "metadata": {},
   "source": [
    "response = user_input(prompt, k_value, session_id, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee22fa-1044-4136-93e1-3ac7538d719b",
   "metadata": {},
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd55d0-085d-406e-9ecb-c9143096ae09",
   "metadata": {},
   "source": [
    "response = user_input(\"me lo explicas de otra forma?\", k_value, session_id, task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb009d14-1cdd-4eaf-b8fc-1e79e494a09c",
   "metadata": {},
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d262d3-e094-4b36-b5d5-58bea30b0d58",
   "metadata": {},
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a0f9310-c7ca-4937-baf1-0678bb70a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "cual es la temperatura en Madrid?\n",
      "{'datasource': 'web_search'}\n",
      "web_search\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mstream(inputs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m         pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1303\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1300\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m _panic_or_proceed(all_futures, loop\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1733\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1731\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1732\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1736\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1738\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\executor.py:59\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m         task\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py:26\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     24\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36mweb_search\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---WEB SEARCH---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m question \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m documents \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Web search\u001b[39;00m\n\u001b[0;32m     19\u001b[0m docs \u001b[38;5;241m=\u001b[39m web_search_tool\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: question})\n",
      "\u001b[1;31mKeyError\u001b[0m: 'documents'"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "\n",
    "inputs = {\"question\": prompt}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e6989-274e-45ba-b140-5fa25c2c18ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
